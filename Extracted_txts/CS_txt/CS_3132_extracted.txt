500

Chapter 17. Making Complex Decisions

 

POLICY

MARKOV DECISION
PROBLEM
MOP

MARKOV PROPERTY

POMDP

assumes that the agent is required to commit to an entire sequence of actions before executing
it. Ifthe agent has no sensors, then this is the best it can do. But if the agent can acquire
new sensory information after each action, then committing to an entire sequence is irrational.
For example, consider the sequence [North,East], starting at (3,2). With probability 0.1, North |
bumps the agent into the wall, leaving it still in (3,2). In this case, carrying on with the sequence
and executing East would be a bad choice.

In reality, the agent will have the opportunity to choose a new action after each step, given
whatever additional information its sensors provide. We therefore need an approach much more
like the conditional planning algorithms of Chapter 13, rather than the search algorithms of
Chapter 3. Ofcourse, these will have to be extended to handle probabilities and utilities. We will
also have to deal with the fact that the "conditional plan" for a stochastic environment may have
to be of infinite size, because it is possible, although unlikely, for the agent to get stuck in one
place (or in a loop) no matter how hard it tries not to.

We begin our analysis with the case of accessible environments. In an accessible envi- |
ronment, the agent's percept at each step will identify the state it is in. If it can calculate the ‘
optimal action for each state, then that will completely determine its behavior. No matter what
the outcome of any action, the agent will always know what to do next

A complete mapping from states to actions is called a policy. Given a policy, it is possible
to calculate the expected utility of the possible environment histories generated by that policy.
The problem, then, is not to calculate the optimal action sequence, but to calculate the optimal
policy—that is, the policy that results in the highest expected utility. An optimal policy for the
world in Figure 17.1 is shown in Figure 17.2(a). Notice that because the cost of taking a step is
fairly small compared to the penalty for ending up in (4,2) by accident, the optimal policy for
the state (3,1) is conservative. The policy recommends taking the long way round, rather than
taking the short cut and thereby risking entering (4,2). As the cost of taking a step is increased,
the optimal policy will, at some point, switch over to the more direct route (see Exercise 17.4).
As the cost ofa step is decreased, the policy will become extremely conservative. For example,
ifthe cost is 0.01, the policy for the state (3,2) is to head West directly into the wall, thereby
avoiding any chance of falling into (4,2).

Once a policy has been calculated from the transition model and the utility function, it
is a trivial matter to decide what to do. A policy represents the agent function explicitly, and
is therefore a description of a simple reflex agent, computed from the information used for a
utility-based agent. Figure 17.3 shows the corresponding agent design.

The problem of calculating an optimal policy in an accessible, stochastic environment
with a known transition model is called a Markov decision problem (MDP), after the Russian
statistician Andrei A. Markov. Markov's work is so closely associated with the assumption of
accessibility, that decision problems are often divided into "Markov" and "non-Markov." More
strictly, we say the Markov property holds if the transition probabilities from any given state
depend only on the state and not on previous history. The next two sections give algorithms for
calculating optimal policies in Markov decision problems.

Tn an inaccessible environment, the percept does not provide enough information to de-
termine the state or the associated transition probabilities. In the operations research literature,
such problems are called partially observable Markov decision problems, or POMDP. Meth-
ods used for MDPs are not directly applicable to POMDPs. For example, suppose our agent

 
