 

19 LEARNING IN NEURAL

AND BELIEF NETWORKS

 

 

 

NEURAL NETWORKS

In which we see how to train complex networks of simple computing elements, thereby
perhaps shedding some light on the workings ofthe brain.

This chapter can be viewed in two ways. From a computational viewpoint, it is about a method
of representing functions using networks of simple arithmetic computing elements, and about
methods for learning such representations from examples. These networks represent functions
in much the same way that circuits consisting of simple logic gates represent Boolean functions.
Such representations are particularly useful for complex functions with continuous-valued outputs
and large numbers of noisy inputs, where the logic-based techniques in Chapter 18 sometimes
have difficulty.

From a biological viewpoint, this chapter is about a mathematical model for the operation
of the brain. The simple arithmetic computing elements correspond to neurons—the cells that
perform information processing in the brain—and the network as a whole corresponds to a
collection of interconnected neurons. For this reason, the networks are called neuralnetworks.'
Besides their useful computational properties, neural networks may offer the best chance of
understanding many psychological phenomena that arise from the specific structure and operation
ofthe brain. We will therefore begin the chapter with a brieflook at what is known about brains,
because this provides much of the motivation for the study of neural networks. In a sense, we
thereby depart from ourintention, stated in Chapter 1, to concentrate on rational action rather than
on imitating humans. These conflicting goals have characterized the study of neural networks
ever since the very first paper on the topic by McCulloch and Pitts (1943). Methodologically
speaking, the goals can be reconciled by acknowledging the fact that humans (and other animals)
do think, and use their powers of thought to act quite successfully in complex domains where
current computer-based agents would be lost. It is instructive to try to see how they do it.

Section 19,2 then presents the idealized models that are the main subject of study. Simple,
single-layer networks called perceptrons are covered in Section 19.3, and general multilayer
networks in Section 19.4. Section 19.5 illustrates the various uses of neural networks.

' Other names that have been used for the field include connectionism, parallel distributed processing, neural

computation, adaptive networks, and collective computation. It should be emphasized that these are artificial neural
networks; there is no attempt to build computing elements out of animal tissue

563
