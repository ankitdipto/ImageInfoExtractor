566

Chapter 19.

Learning in Neural and Belief Networks

 

 

GRACEFUL
DEGRADATION

 

Computer

Human Brain

 

Computational units
Storage units

Cycle time
Bandwidth

Neuron updates/sec

 

 

1 CPU, 10° gates

10° bits RAM, 10!° bits disk
1078 sec

10° bits/sec

10°

 

10'' neurons

10'' neurons, 10'4 synapses
10~3sec

10'* bits/sec

10!

 

 

Figure 192 A crude comparison of the raw computational resources available to computers
(circa 1994) and brains.

 

 

 

case, the difference in storage capacity is minor compared to the difference in switching speed
and in parallelism. Computer chips can execute an instruction in tens of nanoseconds, whereas
neurons require milliseconds to fire. Brains more than make up for this, however, because all
the neurons and synapses are active simultaneously, whereas most current computers have only
one or at most a few CPUs. A neural network running on a serial computer requires hundreds
of cycles to decide ifa single neuron-like unit will fire, whereas in a real brain, al/ the neurons
do this in a single step. Thus, even though a computer is a million times faster in raw switching
speed, the brain ends up being a billion times faster at what it does. One of the attractions of the
neural network approach is the hope that a device could be built that combines the parallelism of
the brain with the switching speed of the computer. Full-scale hardware development will depend
on finding a family of neural network algorithms that provides a basis for long-term investment

A brain can perform a complex task—recognize a face, for example—in less than a second,
which is only enough time for a few hundred cycles. A serial computer requires billions of cycles
to perform the same task less well. Clearly, there is an opportunity for massive parallelism here.
Neural networks may provide a model for massively parallel computation that is more successful
than the approach of "parallelizing" traditional serial algorithms.

Brains are more fault-tolerant than computers. A hardware eiror that flips a single bit
can doom an entire computation, but brain cells die all the time with no ill effect to the overall
functioning of the brain. It is true that there are a variety of diseases and traumas that can affect
a brain, but for the most part, brains manage to muddle through for 70 or 80 years with no need
to replace a memory card, call the manufacturer's service line, or reboot. In addition, brains
are constantly faced with novel input, yet manage to do something with it. Computer programs
rarely work as well with novel input, unless the programmer has been exceptionally careful. The
third attraction of neural networks is graceful degradation: they tend to have a gradual rather
than sharp drop-off in performance as conditions worsen.

The final attraction of neural networks is that they are designed to be trained using an
inductive learning algorithm. (Contrary to the impression given by the popular media, of course,
neural networks are far from being the only AJ systems capable of leaming.) After the network
is initialized, it can be modified to improve its performance on input/output pairs. To the extent
that the learning algorithms can be made general and efficient, this increases the value of neural
networks as psychological models, and makes them useful tools for creating a wide variety of
high-performance applications.

4
:

 
