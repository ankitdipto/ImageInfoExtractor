594 Chapter 19. Learning in Neural and Belief Networks

 

 

BIBLIOGRAPHICAL AND HISTORICAL NOTES

McCulloch and Pitts (1943) introduced the fundamental idea of analyzing neural activity via
thresholds and weighted sums. Early cybernetics and control theory (Wiener, 1948), based on the
notion of negative feedback loops, played a role as a model for learning in neural networks. The
Organization of Behavior (Hebb, 1949) was influential in promoting the hypothesis that human
and animal long-term memory is mediated by permanent alterations in the synapses. Design
for a Brain (Ashby, 1952) put forth the idea that intelligence could be created by the use of
“homeostatic” devices which leam through a kind of exhaustive search.

Minsky and Papert (1988, pp. ix-x) mention a machine built by Marvin Minsky in 1951 that
may well be the first actual neural network learning system ever built. Minsky's (1954) doctoral
dissertation continued the exploration of neural networks. The aptly-named "Pandemonium"
system (Selfridge, 1959; Selfridge and Neisser, 1960) involved a relatively fine-grained distributed
control regime reminiscent of neural networks. Cragg and Temperley (1954; 1955) drew parallels
between McCulloch—Pitts neural networks and "spin systems" in physics. Caianello (1961)
designed a statistical theory of learning in neural networks, drawing on classical statistical
mechanics. Von Neumann (1958) provides a comparison between the functioning of the brain
and the operation ofdigital computers. Frank Rosenblatt (1957) invented the modern “perceptron”
style ofneural network, composed of trainable threshold units.

Similar devices called “adalines” (for "Adaptive Linear") were invented about the same
time (Widrow and Hoff, 1960; Widrow, 1962). Hawkins (1961) gives a detailed history of early
work in "self-organizing systems" or "neural cybernetics," as these approaches were then called.

Frank Rosenblatt (1960) found the first proof of the perceptron convergence theorem,
although it had been foreshadowed by purely mathematical work outside the context of neural
networks (Agmon, 1954; Motzkin and Schoenberg, 1954). Two good books on this period
of research are Neurodynamics (Rosenblatt, 1962) and Learning Machines (Nilsson, 1965).
Nilsson's book is especially comprehensive and detailed. It has recently been republished as The
Mathematical Foundations of Learning Machines (Nilsson, 1990) with a new introduction by
Terrence Sejnowski and Halbert White

Most work in neural networks before 1970 focused on the one-layer perceptron type
of machine, but there were some exceptions. Widrow designed multilayer machines called
“madalines"(Widrow, 1962). Other early multilayer machines are described in (Palmieri and
Sanna, 1960; Gamba et al., 1961).

The publication of Perceptrons (Minsky and Papert, 1969) marked the end of an era. The
authors were severely critical of the unguided experimentation and lack of mathematical rigor
that characterized much of the early work on perceptrons. They established the linear separability
criterion for tasks that could be accomplished by one-layer perceptrons, thus explaining the failure
of the early efforts at solving problems that violated this criterion. Minsky and Papert also gave
some results on early multilayer systems. In the Epilogue to the expanded edition of Perceptrons
(Minsky and Papert, 1988), they forcefully rebut the charge that the publication of the first edition
was responsible for the long perceptron winter of the 1970s, arguing that perceptron research had
already lost its momentum and that the first edition merely explained this phenomenon. They
reaffirm the long-term promise of mathematically sound neural network research, while at the

 
