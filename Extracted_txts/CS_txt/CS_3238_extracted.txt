Section 19.7. Summary 597

 

the input layer I, without any explicit mention to the output of the hidden layer. Show that
there is a network with no hidden units that computes the same function.

b. Repeat the calculation in part (a), this time for a network with any number of hidden layers.
What can you conclude about linear activation functions?

19.4 Considerthe following set ofexamples. Each example has six inputs and one target output:

 

 

 

Hitilllllooooo000
njooolloollololl
bll1loloolloooll
nAjolooloololliloi
ploo0llollollooio
Ki000lo10llollio
TIL 1111101000000

 

 

 

 

a. Run the perceptron learning rule on this example set, and show the resulting set of weights.
b. Run the decision tree learning rule, and show the resulting decision tree
c. Comment on your results

19.5 Implement a data structure for layered, feed-forward neural networks, remembering to
provide the information needed for both forward evaluation and backward propagation. Using
this data structure, write a function NEURAL-NETWORK-OUTPUT that takes an example and a
network and computes the appropriate output values

 

19.6 Suppose that a training set contains only a single example, repeated 100 times. In 80 of
the 100 cases, the single output value is 1; in the other 20, it is 0. What will a back-propagation
network predict for this example, assuming that it has been trained and reaches a global optimum?
(Hint: to find the global optimum, differentiate the exor function and set to zero.)

19.7 The network in Figure 19.13 has four hidden nodes. This number was chosen somewhat
arbitrarily. Run systematic experiments to measure the learning curves for networks with different
numbers of hidden nodes. What is the optimal number? Would it be possible to use a cross-
validation method to find the best network before the fact?
