Section 20.4.

Active Learning in an Unknown Environment 607

 

ROT.

Each adjustment made by ADP could be viewed, from the TD point of view, as a result of
a "pseudo-experience" generated by simulating the current environment model. It is possible to
extend the TD approach to use an environment model to generate several pseudo-experiences—
transitions that the TD agent can imagine might happen given its current model. For each observed
transition, the TD agent can generate a large number of imaginary transitions. In this way, the
resulting utility estimates will approximate more and more closely those of ADP—of course, at
the expense of increased computation time.

Ina similar vein, we can generate more efficient versions of ADP by directly approximating
the algorithms for value iteration or policy iteration. Recall that full value iteration can be
intractable when the number of states is large. Many of the adjustment steps, however, are
extremely tiny. One possible approach to generating reasonably good answers quickly is to
bound the number of adjustments made after each observed transition. One can also use a
heuristic to rank the possible adjustments so as to carry out only the most significant ones. The
prioritized-sweeping heuristic prefers to make adjustments to states whose likely successors
have just undergone a Jarge adjustment in their own utility estimates. Using heuristics like
this, approximate ADP algorithms usually can learn roughly as fast as full ADP, in terms of the
number of training sequences, but can be several orders of magnitude more efficient in terms of
computation (see Exercise 20.3). This enables them to handle state spaces that are far too large
for full ADP. Approximate ADP algorithms have an additional advantage: in the early stages of
learning a new environment, the environment model M often will be far from correct, so there is
little point in calculating an exact utility function to match it. An approximation algorithm can
use a minimum adjustment size that decreases as the environment model becomes more accurate.
This eliminates the very long value iterations that can occur early in learning due to large changes
in the model.

20.4 ACTIVE LEARNING IN AN UNKNOWN ENVIRONMENT

A passive learning agent can be viewed as having a fixed policy, and need not worry about which
actions to take. An active agent must consider what actions to take, what their outcomes may be,
and how they will affect the rewards received.

The PASSIVE-RL-AGENT model of page 602 needs only minor changes to accommodate
actions by the agent:

+ The environment model must now incorporate the probabilities oftransitions to other states
given a particular action. We will use Mito denote the probability of reaching statej if
the action a is taken in state i.

The constraints on the utility ofeach state must now take into account the fact that the agent
has a choice of actions. A rational agent will maximize its expected utility, and instead of
Equation (20.1) we use Equation (17.3), which we repeat here:

U(i) = Rfi) + max S> MZUG) (20.3)
i
