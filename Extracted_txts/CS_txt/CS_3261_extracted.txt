Section 20.7.

Generalization in Reinforcement Learning 617

 

CART-POLE

NOUR

of suitable inductive generalization methods for learning the model. It is also not obvious how
methods such as value and policy iteration can be applied with a generalized model.

We now tum to examples of large-scale applications of reinforcement learning. We will
see that in cases where a utility function (and hence a model) is used, the model is usually taken as
given. For example, in learning an< evaluation function for backgammon, it is normally assumed
that the legal moves, and their effects, are known in advance.

Applications to game-playing

The first significant application of reinforcement learning was also the first significant learning
program of any kindâ€”the checker-playing program written by Arthur Samuel (1959; 1967).
Samuel first used a weighted linear function for the evaluation of positions, using up to 16 terms
at any one time. He applied a version of Equation (20.8) to update the weights. There were some
significant differences, however, between his program and current methods. First, he updated
the weights using the difference between the current state and the backed-up value generated by
full lookahead in the search tree. This works fine, because it amounts to viewing the state space
at a different granularity. A second difference was that the program did not use any observed
rewards! That is, the values ofterminal states were ignored. This means that it is quite possible
for Samuel's program not to converge, or to converge on a strategy designed to lose rather than
win. He managed to avoid this fate by insisting that the weight for material advantage should
always be positive. Remarkably, this was sufficient to direct the program into areas of weight
space corresponding to good checker play (see Chapter 5).

The TD-gammon system (Tesauro, 1992) forcefully illustrates the potential of reinforce-
ment learning techniques. In earlier work (Tesauro and Sejnowski, 1989), Tesauro tried learning
a neural network representation of O(a, i) directly from examples of moves labelled with relative
values by a human expert. This approach proved extremely tedious for the expert. Itresulted ina
program, called Neurogammon, that was strong by computer standards but not competitive with
human grandmasters. The TD-gammon project was an attempt to learn from self-play alone. The
only reward signal was given at the end of each game. The evaluation function was represented
by a fully connected neural network with a single hidden layer containing 40 nodes. Simply by
repeated application of Equation (20.8), TD-gammon learned to play considerably better than
Neurogammon, even though the input representation contained just the raw board position with
no computed features. This took about 200,000 training games and two weeks of computer time
Although this may seem like a lot of games, it is only a vanishingly small fraction of the state
space. When precomputed features were added to the input representation, a network with 80
hidden units was able, after 300,000 training games, to reach a standard of play comparable with
the top three human players worldwide.

Application to robot control

The setup for the famous cart-pole balancing problem, also known as the inverted pendulum,
is shown in Figure 20.14. The problem is to control the position x of the cart so that the pole
stays roughly upright (6 ~ 7/2), while staying within the limits of the cart track as shown.
This problem has been used as a test bed for research in control theory as well as reinforcement
