Section 20.9. Summary 621

 

 

 

 

000110010111 | 8 32% 144010010111 | —»} 111010010111

  
   

 

 

 

 

 

 

 

 

111010101100 ae 000110010111 000140101100 | —>| 000110101100
001110101001 | 6 24% | 411010101400 | | 444040101001 | —+| 111ffho101001

 

 

 

 

ese

my

 

 

 

 

 

 

 

 

 

111011011100 5 20% 001110101001 001110104400. |}—->| 001110101104]
@ () © «d) ©
Initial Population Fitness Function _ Selection Cross-Over Mutation

 

 

 

Figure 20.16 The genetic algorithm. In (a), we have an initial population of 4 individuals.
They are scored by the fitness function in (b); the top individual scores an 8 and the bottom scores
a5. It works out that the top individual has a 32% chance of being chosen on each selection. In
(©), selection has given us two pairs of mates, and the cross-over points (dotted lines) have been
chosen. Notice that one individual mates twice; one not at all. In (d), we see the new offspring,
generated by cross-over of their parents’ genes. Finally, in (e), mutation has changed the two bits
surrounded by boxes. This gives us the population for the next generation.

 

 

 

Like neural networks, genetic algorithms are easy to apply to a wide range of problems. The
results can be very good on some problems, and rather poor on others. In fact, Denker's remark
that "neural networks are the second best way of doing just about anything" has been extended
with "and genetic algorithms are the third." But don't be afraid to try a quick implementation of
a genetic algorithm on a new problem—just to see if it does work—before investing more time
thinking about another approach.

20.9 _ SUMMARY

This chapter has examined the reinforcement learning problem—how an agent can become profi-
cient in an unknown environment given only its percepts and occasional rewards. Reinforcement
learning can be viewed as a microcosm for the entire AI problem, but is studied in a number of
simplified settings to facilitate progress. The following major points were made:

* The overall agent design dictates the kind of information that must be learned. The two
main designs studied are the model-based design, using a model M and a utility function
U, and the model-free approach, using an action-value function Q.

* The utility ofa state is the expected sum of rewards received between now and termination
of the sequence.

* Utilities can be learned using three approaches.

1. The LMS (least-mean-square) approach uses the total observed reward-to-go for a
given state as direct evidence for learning its utility. LMS uses the model only for the
purposes of selecting actions.
