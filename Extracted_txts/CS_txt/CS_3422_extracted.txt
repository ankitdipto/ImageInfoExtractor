 

 

 

 

Section 24.7. Speech Recognition 761
Word Unigram Previous words
count || OF | IN| IS} ON| TO| FROM} THAT) WITH| LINE} VISION
THE 367 179} 143) 44| 44 | 65 35 30 17 0 0
ON 69 0; 0;1) 0); 0 0 0 0 0 0
OF 281 0 0) 2) 0 1 0 3 0 4 0
TO 212 | 0 | 0/19} 0} 0} oO 0 0 0 l
IS 175 0)0);0) 0} 0 0 13 0 1 3
A 153 36 | 36) 33} 23 | 21 4 3 15 0 0
THAT 124 0 3 | 18) 0 1 0 0 0 0 0
WE 105 0; ojfo} 1 0 0 12 0 0 0
LINE 7 1} 0/0) 0 1 0 0 0 0 0
VISION| 13 3 |ofo] ljo 1 0 0 0 0

TRIGRAM

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Figure 2434 A partial table of unigram and bigram counts for the words in this chapter. The
word "the" appears 367 times in all (out of 17613 total words), the bigram "ofthe" appeared 179
times (or about 1%), and the bigram "in the" appeared 143 times. It tums out these are the only
two bigrams that occur more than 100 times.

 

 

 

A big advantage of the bigram model is that it is easy to train the model by counting the
number oftimes each word pair occurs in a representative corpus of strings and using the counts to
estimate the probabilities. For example, if"a" appears 10,000 times in the training corpus and it is
followed by "gun" 37 times, then P(gunj\ai_1) = 37/10, 000, where by P we mean the estimated
probability. After such training one would expect "I have" and "a gun" to have relatively high
estimated probabilities, while "I has" and "an gun" would have low probabilities. One problem
is that the training corpus would probably not contain “gub” at all and more importantly, it would
be missing many valid English words as well, so these words would be assigned an estimated
probability of zero. Therefore, it is customary to set aside a small portion of the probability
distribution for words that do not appear in the training corpus. Figure 24.34 shows some bigram
counts derived from the words in this chapter.

It is possible to go to a trigram model that provides values for P(w;|w;—-1wi—2). This is a
more powerful language model, capable of determining that "ate a banana" is more likely than
“ate a bandana." The problem is that there are so many more parameters in trigram models
that it is hard to get enough training data to come up with accurate probability estimates. A
good compromise is to use a model that consists of a weighted sum of the trigram, bigram, and
unigram (i.e., word frequency) models. The model is defined by the following formula (with
cl 4e34+¢3 = 1):

Pw, ++ -0 ci P(w))+ C2POv;|wi- 1) + €3PGV; Wi 1Wi-2)

  

Bigram or trigram models are not as sophisticated as PCFGs, but they account for local
context-sensitive effects better, and manage to capture some local syntax. For example, the fact
that the word pairs "I has" and "man have" get low scores is reflective of subject-verb agreement.
The problem is that these relationships can only be detected locally: "the man have" gets a low
score, but "the man over there have" is not penalized.
