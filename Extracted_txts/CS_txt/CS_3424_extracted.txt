Section 24.7

Speech Recognition 763

 

 

Word model with dialect variation:

()a)-+GaX © (Ge)

0.5

Word model with coarticulation and dialect variations:

 

 

 

Figure 24.35 Two pronunciation models of the word "tomato." The top one accounts for
dialect differences. The bottom one does that and also accounts for a coarticulation effect.

 

 

 

[m], but all phones will have models with similar topology. A hidden Markov model is just like
a regular Markov model in that it describes a process that goes through a sequence of states. The
difference is that in a regular Markov model, the output is a sequence of state names, and because
each state has a unique name, the output uniquely determines the path through the model. In a
hidden Markov model, each state has a probability distribution of possible outputs, and the same
output can appear in more than one state. HMMs are called hidden models because the true
state of the model is hidden from the observer. In general, when you see that an HMM outputs
some symbol, you can't be sure what state the symbol came from.

Suppose our speech signal is processed to yield the sequence of vector quantization values
[C1,C4,C6]. From the HMM in Figure 24.36, we can compute the probability that this sequence
was generated by the phone [m] as follows. First, we note that there is only one path through the
model that could possibly generate this sequence: the path from Onset to Mid to End, where the
output labels from the three states are C1, C4, and C6, respectively. By looking at the probabilities
on the transition arcs, we see that the probability of this path is 0.7 x 0.1 x 0.6 (these are the
values on the three horizontal arrows in the middle of the Figure 24.36). Next, we look at the
output probabilities for these states to see that the probability of [C 1 ,C4,C6] given this path is
0.5 x 0.7 x 0.5 (these are the values for P(C1|Onser), P(C4|Mid)and P(C6|End), respectively).
So the probability of [C1,C4,C6] given the [m] model is

P((C1, C4, C6]|[m]) = (0.7 x 0.1 x 0.6) x (0.5 x 0.7 x 0.5) = 0.00735

4+ Note that this means that the "tomato" models in Figure 24.35 are actually hidden Markov models, because the same
output (e¢., [f]) appears on more than one state.
