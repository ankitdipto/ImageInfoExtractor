Chapter 26. Philosophical Foundations

 

in 1902. Almost every major figure in the history of logic has at one time or another published
an inconsistent set of axioms. Even today, there is some doubt as to whether the principal
axiomatization of set theory known as ZFC (the Zermelo-Fraenkel axioms plus the Axiom of
Choice) is consistent, even though it is used widely as the basis for most of mathematics. Penrose
replies as follows:

Tam not asserting that, in any particular case ofa formal system F, we need necessarily be able
to "see" that G(F)is true, but I am asserting that the "Godelian insight" that enables one to
pass from F to G(F) isjust as good a mathematical procedure for deriving new truths from old
as are any other procedures in mathematics. This insight is not contained within the rules of F
itself, however. Thus, F does not encapsulate all the insights available to mathematicians. The
insights that are available to mathematicians are not formalizable. (Penrose, 1990, p. 694).

Penrose does not say why he thinks the "Godelian insight" is not formalizable, and it appears
that in fact it has been formalized. In his Ph.D. thesis, Natarajan Shankar (1986) used the
Boyer-Moore theorem prover BMTP to derive Gédel’s theorem from a set of basic axioms, in
much the same way that Gédel himself did.* The thesis was intended mainly to demonstrate
the power of automatic theorem provers, in particular the capability of BMTP to develop and
apply lemmata. But it shows, as Robert Wilensky (1990) has pointed out, that one needs to
be careful to distinguish between the formal system that is doing the proving—in this case, a
computer programmed with the BMTP—and the formal system within which the proofis carried
out, namely, an axiomatization of arithmetic and of sentences in that axiomatization. Just like a
mathematician, a good automated theorem prover can apply Gédelisation to the particular formal
system that it is working on; but it can no more establish the consistency of its own program
and hardware than a mathematician can establish the consistency of his or her own brain. We
therefore seem to be back where we started, with the refutation of the "mathematical objection"
that Turing himself raised.

Penrose naturally maintains that in some way, mathematicians’ use of insight remains
nonalgorithmic. Perhaps the most interesting aspect of his book is the conclusion he draws
from this. After providing a valuable discussion of the relevance of physics to the brain, he
deduces that nothing in our current physical understanding of its operation would suggest that it
has nonalgorithmic aspects—that is, the simulation of its operation by a computer, as described
earlier, would in principle be possible according to modern physics. Rather than accepting the
conclusion that perhaps the brain is, after all, algorithmic in this sense, he prefers to believe
that modern physics must be wrong. In particular, the brain must use physical principles not
yet discovered, but probably relating to the interaction between quantum theory and gravity, that
must also be nonalgorithmic in character.

The argument from informality

One ofthe most influential and persistent criticisms of AI as an enterprise was raised by Turing as
the "argument from informality of behavior." Essentially, this is the claim that human behavior

® Ammon's SHUNYATA system (1993) even appears to have developed by itself the diagonalization technique used by
Gédel and developed originally by Cantor.
