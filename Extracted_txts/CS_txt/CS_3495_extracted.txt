828

Chapter 26. Philosophical Foundations

 

PHENOMENOLOGY

potentially relevant information that could be brought to bear on any given problem. As we saw
in Chapter 22, disambiguation of natural language seems to require access to this background
knowledge. Dreyfus gives as an example the text fragment

Mary saw a dog in the window. She wanted it.

This example was used originally by Lenat (Lenat and Feigenbaum, 1991) to illustrate the
commonsense knowledge needed to disambiguate the "it" in the second sentence. Presumably,
"it" refers to the dog rather than the window. If the second sentence had been "She smashed
it" or "She pressed her nose up against it," the interpretation would be different. To generate
these different interpretations in the different contexts, an AI system would need a fair amount
ofknowledge about dogs, windows, and so on. The project of finding, encoding, and using such
knowledge has been discussed since the early days of AI, and Lenat's CYC project (Lenat and
Guha, 1990) is probably the most well-publicized undertaking in this area.

The position that Dreyfus adopts, however, is that this general commonsense knowledge
is not explicitly represented or manipulated in human performance. It constitutes the "holistic
context" or "Background" within which humans operate. He gives the example of appropriate
social behavior in giving and receiving gifts: "Normally one simply responds in the appropriate
circumstances by giving an appropriate gift." One apparently has "a direct sense of how things
are done and what to expect." The same claim is made in the context of chess playing: "A
mere chess master might need to figure out what to do, but a grandmaster just sees the board as
demanding a certain move." Apparently, the "right response just pops into his or her head."

Dreyfus seems at first to be making a claim that might appear somewhat irrelevant to the
weak AI program: that ifhumans are sometimes not conscious of their reasoning processes, then
on those occasions no reasoning is occurring. The obvious AI reply would be to distinguish
between phenomenology—how things, including our own reasoning, appear to our conscious
experience—and causation. AI is required to find a causal explanation of intelligence. One might
well claim that knowledge of chess—the legal moves and so on—is being used, but perhaps not
at a conscious level. As yet, AI does not claim to have a theory that can distinguish between
conscious and unconscious deliberations, so the phenomenological aspects of decision-making
are unlikely to falsify any particular approach to AI.

Another approach might be to propose that the grandmaster's supposed ability to see the
tight move immediately derives from a partial situation-action mapping used by a reflex agent
with internal state. The mapping might be leamed directly (see Chapter 20) or perhaps compiled
from more explicit knowledge (see Chapter 21). And as discussed in Chapter 2, situation-
action mappings have significant advantages in terms of efficiency. On the other hand, even
a grandmaster sometimes needs to use his or her knowledge of the legal moves to deal with
unfamiliar situations, to find a way out of a trap, or to ensure that a mating attack is unavoidable.

Dreyfus’s position is actually more subtle than a simple appeal to magical intuition. Mind
Over Matter (Dreyfus and Dreyfus, 1986) proposes a five-stage process of acquiring expertise,
beginning with rule-based processing (of the sort proposed in AI) and ending with the ability to
select correct responses instantaneously:

We have seen that computers do indeed reason things out rather like inexperienced persons,

but only with greater human experience comes know-how, a far superior, holistic, intuitive

way of approaching problems that cannot be imitated by rule-following computers.

 

 

 
