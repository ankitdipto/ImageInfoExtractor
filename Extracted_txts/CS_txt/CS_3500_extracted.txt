| 832

Chapter 26. Philosophical Foundations

 

 

inscriptions. (The human therefore plays the role of the CPU, the rule book is the program, and
the stacks of paper are the storage device.) The system is inside a room with a small opening
to the outside. Through the opening appear slips of paper with indecipherable symbols. The
human finds matching symbols in the rule book, and follows the instructions. The instructions
may include writing symbols on new slips of paper, finding symbols in the stacks, rearranging the
stacks, and so on. Eventually, the instructions will cause one or more symbols to be transcribed
onto a piece of paper that is handed through the opening to the outside world.

So far, so good. But from the outside, we see a system that is taking input in the form
of Chinese sentences and generating answers in Chinese that are as obviously "intelligent" as
those in the conversation imagined by Turing.'' Searle then argues as follows: the person in
the room does not understand Chinese (given); the rule book and the stacks of paper, being
just pieces of paper, do not understand Chinese; therefore there is no understanding of Chinese
going on. Hence, According to Searle, running the right program does not necessarily generate

understanding.

Like Turing, Searle considered and attempted to rebuffa number ofreplies to his argument.
First, we will consider the so-called Robot Reply (due to Jerry Fodor (1980) among others), which
tums out to be a red herring, although an interesting one. The Robot Reply is that although the
symbols manipulated by the Chinese Room may not have real meaning to the room itself (e.g.,
nothing in the room has any experience of acupuncture, with respect to which the symbol for it
might have any meaning), a fully equipped robot would not be subject to the same limitations
Its internal symbols would have meaning to it by virtue of its direct experience of the world.
Searle's reply is to put the Chinese Room inside the robot's "head": the sensors are redesigned
to generate Chinese symbols instead of streams of bits, and the effectors redesigned to accept
Chinese symbols as control inputs. Then we are back where we started. The Robot Reply is a
red herring because the causal semantics of the symbols is not the real issue. Even the original
Chinese Room needs some causal semantics, in order to be able to answer questions such as "How
many questions have I asked so far?" Conversely, the outputs ofhuman sensors, for example,
along the optic nerve or the auditory nerve, might as well be in Chinese (see the earlier discussion
of wide and narrow content). Not even Searle would argue that connecting artificial sensors to
these nerves would remove consciousness from the brain involved.!?

Several commentators, including John McCarthy and Robert Wilensky, propose what
Searle calls the Systems Reply. This gets to the point. The objection is that although one can ask
ifthe human in the room understands Chinese, this is analogous to asking if the CPU can take
cube roots. In both cases, the answer is no, and in both cases, according to the Systems Reply,
the entire system does have the capacity in question. Certainly, if one asks the Chinese Room
whether it understands Chinese, the answer would be affirmative (in fluent Chinese). Searle's
response is to reiterate the point that the understanding is not in the human, and cannot be in
the paper, so there cannot be any understanding. He further suggests that one could imagine the
human memorizing the rule book and the contents of all the stacks of paper, so that there would

1 The fact that the stacks of paper might well be larger than the entire planet, and the generation of answers would take
millions of years, has no bearing on the logical structure ofthe argument. One aim of philosophical training is to develop
a finely honed sense of which objections are germane and which are not.

12 This is a good thing, because artificial inner ears are at the prototype stage (Watson, 1991),and artificial retinas, with
associated image processing, are rapidly becoming feasible (Campbell erai., 1991).

 
