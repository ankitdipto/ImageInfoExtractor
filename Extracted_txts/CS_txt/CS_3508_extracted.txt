Section 26.5.

Summary 839

Twentieth-century analytic philosophy has typically accepted physicalism (often in the
form of the brain-state "identity theory" (Place, 1956; Armstrong, 1968), which asserts that
mental states are identical with brain states), but has been much more divided on functionalism,
the machine analogy for the human mind, and the question of whether machines can literally
think. A number of early philosophical responses to Turing's (1950) "Computing Machinery
and Intelligence," for example, Scriven (1953), attempted to deny that it was even meaningful to
say that machines could think, on the ground that such an assertion violated the meanings of the
relevant tems. Scriven, at least, had retracted this view by 1963; see his addendum to a reprint of
his article (Anderson, 1964). In general, later philosophical responses to AI have at least granted
the meaningfulness of the question, although some might answer it vehemently in the negative.

Following the classification used by Block (1980), we can distinguish varieties of func-
tionalism. Functional specification theory (Lewis, 1966; Lewis, 1980) is a variant of brain-state
identity theory that selects the brain states that are to be identified with mental states on the basis
of their functional role. Functional state identity theory (Putnam, 1960; Putnam, 1967) is more
closely based on a machine analogy. It identifies mental states not with physical brain states but
with abstract computational states of the brain conceived expressly as a computing device. These
abstract states are supposed to be independent of the specific physical composition of the brain,
leading some to charge that functional state identity theory is a form of dualism!

Both the brain-state identity theory and the various forms of functionalism have come
under attack from authors who claim that they do not account for the qualia or “what it's
like" aspect of mental states (Nagel, 1974). Searle has focused instead on the alleged inability of
functionalismto account forintentionality (Searle, 1980; Searle, 1984; Searle, 1992). Churchland
and Churchland (1982) rebut both these types of criticism.

Functionalism is the philosophy of mind most naturally suggested by AI, and critiques of
functionalism often take the form of critiques of AI (as in the case of Searle). Other critics of AI,
most notably Dreyfus, have focused specifically on the assumptions and research methods of AI
itself, rather than its general philosophical implications. Even philosophers who are functionalists
are not always sanguine about the prospects of AI as a practical enterprise (Fodor, 1983). Despite
Searle’s “strong”/“weak” terminology, it is possible for a philosopher to believe that human
intellectual capabilities could in principle be duplicated by duplicating their functional structure
alone (and thus to support "strong AI") while also believing that as a practical matter neither GOFAI
nor any other human endeavor is likely to discover that functional structure in the foreseeable
future (and thus to oppose "weak AI"). In fact, this seems to be a relatively common viewpoint
among philosophers.

Not all philosophers are critical of GOFAI, however; some are, in fact, ardent advocates
and even practitioners. Zenon Pylyshyn (1984) has argued that cognition can best be understood
through a computational model, not only in principle but also as a way of conducting research
at present, and has specifically rebutted Dreyfus's criticisms of the computational model of
human cognition (Pylyshyn, 1974). Gilbert Harman (1983), in analyzing beliefrevision, makes
connections with AI research on truth maintenance systems. Michael Bratman has applied
his "belief-desire-intention" model of human psychology (Bratman, 1987) to AI research on
planning (Bratman, 1992). At the extreme end of strong AI, Aaron Sloman (1978, p. xiii) has
even described as "racialist" Joseph Weizenbaum's view (Weizenbaum, 1976) that hypothetical
intelligent machines should not be regarded as persons.

 
