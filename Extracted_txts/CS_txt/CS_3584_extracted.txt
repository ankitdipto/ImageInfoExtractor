12

Chapter 1. Tntroduction

 

REDUCTION

NP COMPLETENESS

DECISION THEORY

stances cannot be solved in any reasonable time. Therefore, one should strive to divide the overall
problem of generating intelligent behavior into tractable subproblems rather than intractable ones.
The second important concept in the theory of complexity is reduction, which also emerged in
the 1960s (Dantzig, 1960; Edmonds, 1962). A reduction is a general transformation from one
class of problems to another, such that solutions to the first class can be found by reducing them
to problems of the second class and solving the latter problems.

How can one recognize an intractable problem? The theory of NP-completeness, pioneered
by Steven Cook (1971) and Richard Karp (1972), provides a method. Cook and Karp showed
the existence of large classes of canonical combinatorial search and reasoning problems that
are NP-complete. Any problem class to which an NP-complete problem class can be reduced
is likely to be intractable. (Although it has not yet been proved that NP-complete problems
are necessarily intractable, few theoreticians believe otherwise.) These results contrast sharply
with the "Electronic Super-Brain" enthusiasm accompanying the advent of computers. Despite
the ever-increasing speed of computers, subtlety and careful use of resources will characterize
intelligent systems. Put crudely, the world is an extremely large problem instance!

Besides logic and computation, the third great contribution of mathematics to Al is the j
theory of probability. The Italian Gerolamo Cardano (1501-1576) first framed the idea of
probability, describing it in terms of the possible outcomes of gambling events. Before his time,
the outcomes of gambling games were seen as the will ofthe gods rather than the whim of chance.
Probability quickly became an invaluable part of all the quantitative sciences, helping to deal
with uncertain measurements andincomplete theories. Pierre Fermat (1601-1665), Blaise Pascal
(1623-1662), James Bernoulli (1654-1705), Pierre Laplace (1749-1827), and others advanced j
the theory and introduced new statistical methods. Bernoulli also framed an alternative view
of probability, as a subjective "degree of belief" rather than an objective ratio of outcomes.
Subjective probabilities therefore can be updated as new evidence is obtained. Thomas Bayes.
(1702-1761) proposed a rule for updating subjective probabilities in the light of new evidence
(published posthumously in 1763). Bayesâ€™ rule, and the subsequent field of Bayesian analysis,
form the basis of the modem approach to uncertain reasoning in AI systems. Debate still rages j
between supporters of the objective and subjective views of probability, but it is not clear if the
difference has great significance for AI. Both versions obey the same set of axioms. Savage's.
(1954) Foundations of Statistics gives a good introduction to the field.

As with logic, a connection must be made between probabilistic reasoning and action.
Decision theory, pioneered by John Von Neumann and Oskar Morgenstern (1944), combines:
probability theory with utility theory (which provides a formal and complete framework for!
specifying the preferences of an agent) to give the first general theory that can distinguish good
actions from bad ones. Decision theory is the mathematical successor to utilitarianism, and
provides the theoretical basis for many of the agent designs in this book.

  
  
   
   
   
  
    
   

6EH

96
Psi

Psychology (1879-present)

Scientific psychology can be said to have begun with the work of the German physicist Hermann j
von Helmholtz (1821-1894) and his student Wilhelm Wundt (1832-1920). Helmholtz applied
the scientific method to the study of human vision, and his Handbook of Physiological Optics
