Section 1.3.

The History of Artificial Intelligence 19

 

wig WORLDS

logic included Cordell Green's question answering and planning systems (Green, 1969b), and the
Shakey robotics project at the new Stanford Research Institute (SRI). The latter project, discussed
further in Chapter 25, was the first to demonstrate the complete integration of logical reasoning
and physical activity.

Minsky supervised a series of students who chose limited problems that appeared to require
intelligence to solve. These limited domains became known as microworlds. James Slagle's
SAINT program (1963a) was able to solve closed-form integration problems typical of first-year
college calculus courses. Tom Evans's ANALOGY program (1968) solved geometric analogy
problems that appear in 1Q tests, such as the one in Figure 1.2. Bertram Raphael's (1968) SIR
(Semantic Information Retrieval) was able to accept input statements in a very restricted subset
of English and answer questions thereon. Daniel Bobrowâ€™s STUDENT program (1967) solved
algebra story problems such as

If the number of customers Tom gets is twice the square of 20 percent of the number of

advertisements he runs, and the number of advertisements he runs is 45, what is the number
of customers Tom gets?

 

 

 

is to as is to:

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

1 2 3 4 5

 

 

 

 

Figure 12 An example problem solved by Evans's ANALOGY program.

 

The most famous microworld was the blocks world, which consists of a set of solid blocks
placed on a tabletop (or more often, a simulation of a tabletop), as shown in Figure 1.3. A task
in this world is to rearrange the blocks in a certain way, using a robot hand that can pick up one
block at a time. The blocks world was home to the vision project of David Huffman (1971),
the vision and constraint-propagation work of David Waltz (1975), the leaming theory of Patrick
Winston (1970), the natural language understanding program of Terry Winograd (1972), and the
planner of Scott Fahlman (1974).

Early work building on the neural networks of McCulloch and Pitts also flourished. The
work of Winograd and Cowan (1963) showed how a large number of elements could collectively
represent an individual concept, with a corresponding increase in robustness and parallelism.
Hebb's learning methods were enhanced by Bernie Widrow (Widrow and Hoff, 1960; Widrow,

1962), who called his networks adalines, and by Frank Rosenblatt (1962) with his perceptrons.
