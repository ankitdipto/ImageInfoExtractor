{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /opt/conda/bin/python3.7 -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow-gpu==1.14.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow==1.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tensorflow.keras import Sequential,layers\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import random\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath2='../input/musicdata/Digital_Music_5.json'\n",
    "filePathLocal='datasets/Digital_Music_5.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_json(filePathLocal,lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A3EBHHCZO6V2A4</td>\n",
       "      <td>5555991584</td>\n",
       "      <td>Amaranth \"music fan\"</td>\n",
       "      <td>[3, 3]</td>\n",
       "      <td>It's hard to believe \"Memory of Trees\" came ou...</td>\n",
       "      <td>5</td>\n",
       "      <td>Enya's last great album</td>\n",
       "      <td>1158019200</td>\n",
       "      <td>09 12, 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AZPWAXJG9OJXV</td>\n",
       "      <td>5555991584</td>\n",
       "      <td>bethtexas</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>A clasically-styled and introverted album, Mem...</td>\n",
       "      <td>5</td>\n",
       "      <td>Enya at her most elegant</td>\n",
       "      <td>991526400</td>\n",
       "      <td>06 3, 2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A38IRL0X2T4DPF</td>\n",
       "      <td>5555991584</td>\n",
       "      <td>bob turnley</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>I never thought Enya would reach the sublime h...</td>\n",
       "      <td>5</td>\n",
       "      <td>The best so far</td>\n",
       "      <td>1058140800</td>\n",
       "      <td>07 14, 2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A22IK3I6U76GX0</td>\n",
       "      <td>5555991584</td>\n",
       "      <td>Calle</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>This is the third review of an irish album I w...</td>\n",
       "      <td>5</td>\n",
       "      <td>Ireland produces good music.</td>\n",
       "      <td>957312000</td>\n",
       "      <td>05 3, 2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A1AISPOIIHTHXX</td>\n",
       "      <td>5555991584</td>\n",
       "      <td>Cloud \"...\"</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>Enya, despite being a successful recording art...</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5; music to dream to</td>\n",
       "      <td>1200528000</td>\n",
       "      <td>01 17, 2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64701</th>\n",
       "      <td>A1PQ1PESSO8CMO</td>\n",
       "      <td>B00KILDVEI</td>\n",
       "      <td>Ginger Christmas</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I like the reggae sound a lot in this song. I ...</td>\n",
       "      <td>4</td>\n",
       "      <td>Cool song</td>\n",
       "      <td>1403568000</td>\n",
       "      <td>06 24, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64702</th>\n",
       "      <td>A120RH58WVY4W6</td>\n",
       "      <td>B00KILDVEI</td>\n",
       "      <td>Kelly Dunwell \"avid reader\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I first heard this on Sirius and had to have i...</td>\n",
       "      <td>5</td>\n",
       "      <td>Great Song</td>\n",
       "      <td>1404864000</td>\n",
       "      <td>07 9, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64703</th>\n",
       "      <td>A19VJ2IQLO50G0</td>\n",
       "      <td>B00KILDVEI</td>\n",
       "      <td>melinda</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>I absolutely love this song, it downloaded fin...</td>\n",
       "      <td>5</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1405209600</td>\n",
       "      <td>07 13, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64704</th>\n",
       "      <td>AUDSM2CTLLW1Q</td>\n",
       "      <td>B00KILDVEI</td>\n",
       "      <td>Patrick L. Randall</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>Reggae, island beats aren't really my cup of t...</td>\n",
       "      <td>3</td>\n",
       "      <td>Well-crafted song</td>\n",
       "      <td>1404864000</td>\n",
       "      <td>07 9, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64705</th>\n",
       "      <td>A1GN8UJIZLCA59</td>\n",
       "      <td>B00KILDVEI</td>\n",
       "      <td>P Magnum</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>Magic! is a Canadian band that incorporates re...</td>\n",
       "      <td>1</td>\n",
       "      <td>Souless Reggae</td>\n",
       "      <td>1405641600</td>\n",
       "      <td>07 18, 2014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64706 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           reviewerID        asin                 reviewerName helpful  \\\n",
       "0      A3EBHHCZO6V2A4  5555991584         Amaranth \"music fan\"  [3, 3]   \n",
       "1       AZPWAXJG9OJXV  5555991584                    bethtexas  [0, 0]   \n",
       "2      A38IRL0X2T4DPF  5555991584                  bob turnley  [2, 2]   \n",
       "3      A22IK3I6U76GX0  5555991584                        Calle  [1, 1]   \n",
       "4      A1AISPOIIHTHXX  5555991584                  Cloud \"...\"  [1, 1]   \n",
       "...               ...         ...                          ...     ...   \n",
       "64701  A1PQ1PESSO8CMO  B00KILDVEI             Ginger Christmas  [0, 0]   \n",
       "64702  A120RH58WVY4W6  B00KILDVEI  Kelly Dunwell \"avid reader\"  [0, 0]   \n",
       "64703  A19VJ2IQLO50G0  B00KILDVEI                      melinda  [0, 1]   \n",
       "64704   AUDSM2CTLLW1Q  B00KILDVEI           Patrick L. Randall  [0, 0]   \n",
       "64705  A1GN8UJIZLCA59  B00KILDVEI                     P Magnum  [1, 2]   \n",
       "\n",
       "                                              reviewText  overall  \\\n",
       "0      It's hard to believe \"Memory of Trees\" came ou...        5   \n",
       "1      A clasically-styled and introverted album, Mem...        5   \n",
       "2      I never thought Enya would reach the sublime h...        5   \n",
       "3      This is the third review of an irish album I w...        5   \n",
       "4      Enya, despite being a successful recording art...        4   \n",
       "...                                                  ...      ...   \n",
       "64701  I like the reggae sound a lot in this song. I ...        4   \n",
       "64702  I first heard this on Sirius and had to have i...        5   \n",
       "64703  I absolutely love this song, it downloaded fin...        5   \n",
       "64704  Reggae, island beats aren't really my cup of t...        3   \n",
       "64705  Magic! is a Canadian band that incorporates re...        1   \n",
       "\n",
       "                            summary  unixReviewTime   reviewTime  \n",
       "0           Enya's last great album      1158019200  09 12, 2006  \n",
       "1          Enya at her most elegant       991526400   06 3, 2001  \n",
       "2                   The best so far      1058140800  07 14, 2003  \n",
       "3      Ireland produces good music.       957312000   05 3, 2000  \n",
       "4            4.5; music to dream to      1200528000  01 17, 2008  \n",
       "...                             ...             ...          ...  \n",
       "64701                     Cool song      1403568000  06 24, 2014  \n",
       "64702                    Great Song      1404864000   07 9, 2014  \n",
       "64703                    Five Stars      1405209600  07 13, 2014  \n",
       "64704             Well-crafted song      1404864000   07 9, 2014  \n",
       "64705                Souless Reggae      1405641600  07 18, 2014  \n",
       "\n",
       "[64706 rows x 9 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdf=df[[\"reviewText\",\"summary\",\"overall\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It's hard to believe \"Memory of Trees\" came ou...</td>\n",
       "      <td>Enya's last great album</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A clasically-styled and introverted album, Mem...</td>\n",
       "      <td>Enya at her most elegant</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I never thought Enya would reach the sublime h...</td>\n",
       "      <td>The best so far</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is the third review of an irish album I w...</td>\n",
       "      <td>Ireland produces good music.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Enya, despite being a successful recording art...</td>\n",
       "      <td>4.5; music to dream to</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText  \\\n",
       "0  It's hard to believe \"Memory of Trees\" came ou...   \n",
       "1  A clasically-styled and introverted album, Mem...   \n",
       "2  I never thought Enya would reach the sublime h...   \n",
       "3  This is the third review of an irish album I w...   \n",
       "4  Enya, despite being a successful recording art...   \n",
       "\n",
       "                        summary  overall  \n",
       "0       Enya's last great album        5  \n",
       "1      Enya at her most elegant        5  \n",
       "2               The best so far        5  \n",
       "3  Ireland produces good music.        5  \n",
       "4        4.5; music to dream to        4  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=subdf['summary'].values\n",
    "y=subdf['overall'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19763 unique tokens.\n",
      "(64706, 100)\n",
      "(64706, 6)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MAX_NUM_WORDS = 1000\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = to_categorical(subdf['overall'].values)\n",
    "print(data.shape)\n",
    "print(labels.shape)                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(data))\n",
    "print(type(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0 131  10   4]\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocabulary=tokenizer.get_config()\n",
    "#print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(vocabulary['word_counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train, sentences_test, y_train, y_test=\\\n",
    "train_test_split(data,labels,test_size=0.20,random_state=62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#vectorizer=CountVectorizer(min_df=0, lowercase=False)\n",
    "#vectorizer.fit(sentences_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(sentences_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train=vectorizer.transform(sentences_train)\n",
    "#X_test=vectorizer.transform(sentences_test)\n",
    "#y_train=y_train.reshape(-1,1)\n",
    "#y_test=y_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_train.shape,y_train.shape)\n",
    "#print(vectorizer.vocabulary_)\n",
    "#print(len(vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one_hot_encoder=OneHotEncoder(sparse=False)\\none_hot_encoder.fit(y_train)\\nencoded_y_train=one_hot_encoder.transform(y_train)\\nencoded_y_test=one_hot_encoder.transform(y_test)'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"one_hot_encoder=OneHotEncoder(sparse=False)\n",
    "one_hot_encoder.fit(y_train)\n",
    "encoded_y_train=one_hot_encoder.transform(y_train)\n",
    "encoded_y_test=one_hot_encoder.transform(y_test)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(encoded_y_train)\n",
    "#print(encoded_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(type(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clfBase=LogisticRegression(max_iter=1400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clfBase.fit(X_train,encoded_y_train)\n",
    "#scoreBase=clfBase.score(X_test,encoded_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(scoreBase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clfNN=Sequential()\n",
    "clfNN.add(layers.Dense(7,input_dim=sentences_train.shape[1],activation='sigmoid'))\n",
    "clfNN.add(layers.Dense(8,activation='sigmoid'))\n",
    "clfNN.add(layers.Dense(6,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfNN.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 7)                 707       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 64        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 54        \n",
      "=================================================================\n",
      "Total params: 825\n",
      "Trainable params: 825\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "clfNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!rm -r ./logs/fit/*\\nlogDirPath=\\'logs/fit/\\'+datetime.now().strftime(\"%Y%m%d-%H%M%S\")\\ntensorboardCallback=tf.keras.callbacks.TensorBoard(log_dir=logDirPath,histogram_freq=1)'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"!rm -r ./logs/fit/*\n",
    "logDirPath='logs/fit/'+datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboardCallback=tf.keras.callbacks.TensorBoard(log_dir=logDirPath,histogram_freq=1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51764, 6)\n",
      "(51764, 100)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(sentences_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 51764 samples\n",
      "51764/51764 [==============================] - 3s 50us/sample - loss: 1.2555 - acc: 0.5352\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa7912eb710>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfNN.fit(sentences_train,y_train)\n",
    "          \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.548509 and Loss: 1.1946890\n",
      "Testing  Accuracy: 0.555324 and Loss: 1.1880978\n"
     ]
    }
   ],
   "source": [
    "loss,accuracy=clfNN.evaluate(sentences_train,y_train,verbose=False)\n",
    "print(\"Training Accuracy: {:.6f} and Loss: {:.7f}\".format(accuracy,loss))\n",
    "loss,accuracy=clfNN.evaluate(sentences_test,y_test,verbose=False)\n",
    "print(\"Testing  Accuracy: {:.6f} and Loss: {:.7f}\".format(accuracy,loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(type(X_train))\n",
    "#tf.sparse.to_dense(X_train)\n",
    "#print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_26 (Embedding)     (None, 100, 37)           37000     \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 3700)              0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 6)                 22206     \n",
      "=================================================================\n",
      "Total params: 59,206\n",
      "Trainable params: 59,206\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim=MAX_NUM_WORDS,output_dim=37,input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "#model.add(layers.GlobalAveragePooling1D())\n",
    "model.add(layers.Dense(6, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# summarize the model\n",
    "print(type(model.layers[0].output))\n",
    "print(type(model.layers[1].output))\n",
    "print(type(model.layers[2].output))\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 51764 samples\n",
      "51764/51764 [==============================] - 15s 293us/sample - loss: 1.0756 - acc: 0.5767\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa79138af98>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(sentences_train,y_train,batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12942/12942 [==============================] - 1s 72us/sample - loss: 1.0205 - acc: 0.6018\n",
      "1.0205200089219733 0.60183895\n"
     ]
    }
   ],
   "source": [
    "loss,accuracy=model.evaluate(sentences_test,y_test)\n",
    "print(loss,accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def trainInBatch(model,batch_size,X_train,y_train):\n",
    "    print(\"No. of training examples: \",X_train.shape[0])\n",
    "    numOfBatches=X_train.shape[0]//batch_size\n",
    "    for idx in range(numOfBatches):\n",
    "        start=idx*batch_size\n",
    "        stop=min((idx+1)*batch_size,X_train.shape[0])\n",
    "        model.fit(X_train[start:stop][:],y_train[start:stop])\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model,X,y,batch_size):\n",
    "    y_pred=np.zeros(y.shape)\n",
    "    numOfBatches=X.shape[0]//batch_size\n",
    "    cnt=0\n",
    "    for idx in range(numOfBatches):\n",
    "        start=idx*batch_size\n",
    "        stop=min((idx+1)*batch_size,X.shape[0])\n",
    "        y_pred[start:stop][:]=model.predict(X[start:stop][:])\n",
    "    return (y_pred==y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51764/51764 [==============================] - 2s 45us/sample - loss: 1.1947 - acc: 0.5485\n",
      "Training Accuracy:  [1.1946890066381346, 0.54850864]\n",
      "12942/12942 [==============================] - 1s 44us/sample - loss: 1.1881 - acc: 0.5553\n",
      "Testing  Accuracy:  [1.188097841311157, 0.5553238]\n"
     ]
    }
   ],
   "source": [
    "result2=clfNN.evaluate(sentences_train,y_train)\n",
    "print(\"Training Accuracy: \",result2)\n",
    "result2=clfNN.evaluate(sentences_test,y_test)\n",
    "print(\"Testing  Accuracy: \",result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_32\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_30 (Embedding)     (None, 100, 38)           38000     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_23 (Glo (None, 38)                0         \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 11)                429       \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 6)                 72        \n",
      "=================================================================\n",
      "Total params: 38,501\n",
      "Trainable params: 38,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelPool=Sequential()\n",
    "modelPool.add(layers.Embedding(input_dim=MAX_NUM_WORDS,\n",
    "                               output_dim=38,\n",
    "                               input_length=MAX_SEQUENCE_LENGTH))\n",
    "modelPool.add(layers.GlobalMaxPool1D())\n",
    "modelPool.add(layers.Dense(11,activation=\"relu\"))\n",
    "modelPool.add(layers.Dense(6,activation=\"sigmoid\"))\n",
    "modelPool.compile(optimizer='adam',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=[\"accuracy\"])\n",
    "modelPool.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 51764 samples\n",
      "Epoch 1/12\n",
      "51764/51764 [==============================] - 17s 321us/sample - loss: 1.0996 - acc: 0.5655\n",
      "Epoch 2/12\n",
      "51764/51764 [==============================] - 15s 299us/sample - loss: 1.0045 - acc: 0.5955\n",
      "Epoch 3/12\n",
      "51764/51764 [==============================] - 15s 285us/sample - loss: 0.9811 - acc: 0.6026\n",
      "Epoch 4/12\n",
      "51764/51764 [==============================] - 15s 291us/sample - loss: 0.9662 - acc: 0.6076\n",
      "Epoch 5/12\n",
      "51764/51764 [==============================] - 16s 317us/sample - loss: 0.9536 - acc: 0.6132\n",
      "Epoch 6/12\n",
      "51764/51764 [==============================] - 17s 336us/sample - loss: 0.9410 - acc: 0.6182\n",
      "Epoch 7/12\n",
      "51764/51764 [==============================] - 18s 356us/sample - loss: 0.9283 - acc: 0.6240\n",
      "Epoch 8/12\n",
      "51764/51764 [==============================] - 17s 338us/sample - loss: 0.9151 - acc: 0.6288\n",
      "Epoch 9/12\n",
      "51764/51764 [==============================] - 18s 348us/sample - loss: 0.9016 - acc: 0.6340\n",
      "Epoch 10/12\n",
      "51764/51764 [==============================] - 21s 407us/sample - loss: 0.8877 - acc: 0.6413\n",
      "Epoch 11/12\n",
      "51764/51764 [==============================] - 19s 358us/sample - loss: 0.8743 - acc: 0.6465\n",
      "Epoch 12/12\n",
      "51764/51764 [==============================] - 18s 341us/sample - loss: 0.8609 - acc: 0.6547\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa7916d0da0>"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelPool.fit(sentences_train,y_train,\n",
    "             batch_size=10,\n",
    "             epochs=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy= 0.5963\n",
      "Testing accuracy= 0.5987\n"
     ]
    }
   ],
   "source": [
    "loss,accuracy=modelPool.evaluate(sentences_train,y_train,verbose=False)\n",
    "print(\"Training accuracy= {:.4f}\".format(accuracy))\n",
    "loss,accuracy=modelPool.evaluate(sentences_test,y_test,verbose=False)\n",
    "print(\"Testing accuracy= {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(filepath,word_index,embedding_dim):\n",
    "    vocab_size=len(word_index)+1\n",
    "    embedding_matrix=np.zeros((vocab_size,embedding_dim))\n",
    "    \n",
    "    f=open(filepath)\n",
    "    for line in f:\n",
    "        word,*vector=line.split()\n",
    "        if word in word_index:\n",
    "            index=word_index[word]\n",
    "            embedding_matrix[index]=np.array(vector, dtype=np.float32)[:embedding_dim]\n",
    "    f.close()\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=50\n",
    "embedding_matrix=create_embedding_matrix('datasets/glove.6B.50d.txt',\n",
    "                                         tokenizer.word_index,\n",
    "                                         embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of words covered by the pretrained word embeddings: 0.76832\n"
     ]
    }
   ],
   "source": [
    "vocab_size=len(tokenizer.word_index)+1\n",
    "nonzero_elements=np.count_nonzero(np.count_nonzero(embedding_matrix,axis=1))\n",
    "print(\"Percentage of words covered by the pretrained word embeddings: {:.5f}\".format(nonzero_elements/vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_30\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_28 (Embedding)     (None, 100, 50)           988200    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_21 (Glo (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 8)                 408       \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 6)                 54        \n",
      "=================================================================\n",
      "Total params: 988,662\n",
      "Trainable params: 462\n",
      "Non-trainable params: 988,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelG=Sequential()\n",
    "modelG.add(layers.Embedding(vocab_size,embedding_dim,\n",
    "                           weights=[embedding_matrix],\n",
    "                           input_length=MAX_SEQUENCE_LENGTH,\n",
    "                           trainable=False))\n",
    "modelG.add(layers.GlobalMaxPool1D())\n",
    "modelG.add(layers.Dense(8,activation=\"relu\"))\n",
    "modelG.add(layers.Dense(6,activation=\"sigmoid\"))\n",
    "modelG.compile(optimizer=\"adam\",\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "modelG.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 51764 samples\n",
      "Epoch 1/12\n",
      "51764/51764 [==============================] - 16s 310us/sample - loss: 0.9974 - acc: 0.6001\n",
      "Epoch 2/12\n",
      "51764/51764 [==============================] - 16s 310us/sample - loss: 0.9681 - acc: 0.6100\n",
      "Epoch 3/12\n",
      "51764/51764 [==============================] - 18s 346us/sample - loss: 0.9432 - acc: 0.6194\n",
      "Epoch 4/12\n",
      "51764/51764 [==============================] - 20s 378us/sample - loss: 0.9192 - acc: 0.6270\n",
      "Epoch 5/12\n",
      "51764/51764 [==============================] - 17s 327us/sample - loss: 0.8997 - acc: 0.6329\n",
      "Epoch 6/12\n",
      "51764/51764 [==============================] - 17s 320us/sample - loss: 0.8822 - acc: 0.6412\n",
      "Epoch 7/12\n",
      "51764/51764 [==============================] - 17s 330us/sample - loss: 0.8678 - acc: 0.6456\n",
      "Epoch 8/12\n",
      "51764/51764 [==============================] - 17s 335us/sample - loss: 0.8537 - acc: 0.6507\n",
      "Epoch 9/12\n",
      "51764/51764 [==============================] - 17s 337us/sample - loss: 0.8417 - acc: 0.6556\n",
      "Epoch 10/12\n",
      "51764/51764 [==============================] - 17s 319us/sample - loss: 0.8312 - acc: 0.6584\n",
      "Epoch 11/12\n",
      "51764/51764 [==============================] - 17s 334us/sample - loss: 0.8210 - acc: 0.6624\n",
      "Epoch 12/12\n",
      "51764/51764 [==============================] - 16s 316us/sample - loss: 0.8127 - acc: 0.6645\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa7916d0ef0>"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(sentences_train,y_train,batch_size=10,epochs=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51764/51764 [==============================] - 3s 57us/sample - loss: 1.7123 - acc: 0.3853\n",
      "Training accuracy= 0.3853\n",
      "12942/12942 [==============================] - 1s 55us/sample - loss: 1.7106 - acc: 0.3957\n",
      "Testing accuracy= 0.3957\n"
     ]
    }
   ],
   "source": [
    "loss,accuracy=modelG.evaluate(sentences_train,y_train)\n",
    "print(\"Training accuracy= {:.4f}\".format(accuracy))\n",
    "loss,accuracy=modelG.evaluate(sentences_test,y_test)\n",
    "print(\"Testing accuracy= {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not np.any(np.isnan(sentences_train))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
